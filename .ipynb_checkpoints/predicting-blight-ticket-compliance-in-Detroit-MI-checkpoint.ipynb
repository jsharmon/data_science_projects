{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all of the relevant csv files\n",
    "train = pd.read_csv('data/train.csv', encoding = \"ISO-8859-1\")\n",
    "test = pd.read_csv('data/test.csv')\n",
    "addresses = pd.read_csv('data/addresses.csv')\n",
    "latlons = pd.read_csv('data/latlons.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NULL compliance values to leave binary classification problem\n",
    "train.dropna(subset=['compliance'], inplace=True)\n",
    "\n",
    "# Drop a number of features for reasons listed:\n",
    "# Removing collection status and compliance detail to avoid data leakage. Removing violator name because\n",
    "# it doesn't seem like that would provide much generalizable information. Removing information about violation\n",
    "# location, replacing with latitude/longitude. Remove fine_amount, admin_fee, state_fee as they're rolled into\n",
    "# judgment_amount, but keep the late_fee, discount_amount, and clean_up_cost. Maybe get rid of clean_up_cost\n",
    "# later. Remove other columns related to payment, prevent data leakage. Removing mailing address st name and\n",
    "# zip code, as well as non_us_str_code. Removing violation_description as it should overlap with violation_code.\n",
    "# Removing city as I was killing the kernel trying to one hot encode it. Removing grafitti_status as the entire\n",
    "# column was NaN.\n",
    "droplist = ['violator_name', 'violation_street_number', 'violation_street_name', \n",
    "            'violation_zip_code', 'fine_amount','admin_fee','state_fee',\n",
    "            'payment_amount', 'payment_date', 'payment_status', 'balance_due',\n",
    "            'collection_status', 'compliance_detail', \n",
    "            'mailing_address_str_name', 'zip_code', 'non_us_str_code',\n",
    "            'violation_description', 'city', 'grafitti_status']\n",
    "train.drop(droplist, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge address and lat lon, then merge into main df\n",
    "addFull = pd.merge(addresses, latlons, on='address')\n",
    "\n",
    "# Remove address as the info there is already covered by lat and lon\n",
    "X = pd.merge(train, addFull, on='ticket_id').drop('address', axis=1)\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "categoricalCols = ['agency_name', 'state', 'country', 'disposition', \n",
    "                   'violation_code', 'inspector_name']\n",
    "\n",
    "for col in categoricalCols:\n",
    "    X = pd.concat([X.drop(col, axis=1), pd.get_dummies(X[col])], axis=1)\n",
    "\n",
    "# Convert datetime columns to seconds since the epoch\n",
    "dateCols = ['ticket_issued_date', 'hearing_date']\n",
    "\n",
    "for col in dateCols:\n",
    "    X[col] = pd.to_datetime(X[col])\n",
    "    X[col] = (X[col] - datetime.datetime(1970,1,1)).dt.total_seconds()\n",
    "\n",
    "# Introduce new feature - time from ticket issue to hearing date, remove old cols\n",
    "X['time_to_hearing'] = X['hearing_date'] - X['ticket_issued_date']\n",
    "X.drop(['ticket_issued_date', 'hearing_date'], axis=1, inplace=True)\n",
    "    \n",
    "# Do a final dropna\n",
    "X.dropna(inplace=True)\n",
    "\n",
    "# Get target values, then drop col from features\n",
    "y = X['compliance']\n",
    "X.drop('compliance', axis=1, inplace=True)\n",
    "\n",
    "# Remove ticket id as this likely is not informative for future cases\n",
    "X.drop('ticket_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the training data. Won't do this for actual function, but is useful here for evaluation.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "rfc = RandomForestClassifier(max_features = 8, n_estimators = 10).fit(X_train, y_train)\n",
    "\n",
    "# Actually looks surpisingly good!!\n",
    "print('Training accuracy: {}'.format(rfc.score(X_train, y_train)))\n",
    "print('Test accuracy: {}'.format(rfc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_curve, auc\n",
    "\n",
    "print(classification_report(y_test, rfc.predict(X_test), target_names=['non-compliant', 'compliant']))\n",
    "\n",
    "fpr, tpr, _ = roc_curve(np.asarray(y_test), rfc.predict_proba(X_test)[:,1])\n",
    "print('AUC: {}'.format(auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(rfc.feature_importances_, index = X_train.columns,\n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
    "feature_importances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
